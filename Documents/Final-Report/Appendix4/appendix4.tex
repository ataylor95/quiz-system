\chapter{User survey results}
\label{appendix:user-results}
Questionnaires were sent out to the students and lecturer via Google Forms, with the consent form as the description of the questionnaire.

For all the raw results or an example questionnaire, please contact the author.

\section{Desktop}
There were eleven questionnaires completed for desktop users. Overall the users responded positively. All users stated that questions and non question slides were easy to read, and only one stated that the answers were not easy to click on and submit. They all agreed that the pages updated within reasonable time when the lecturer changed question, which was less than five seconds. When asked to rate the UI of their view and the results view on the lecturers screen, all gave either four or five out of five. They also all rated the overall experience fours and fives.
\section{Mobile}
There were unfortunately only two respondents meaning that there is no clear consensus. The two responses had very differing views, with one agreeing that the UI was easy to read and use whilst the other disagreed. They both agreed that the pages did not update within a reasonable time however, suggesting a potential issue with the WebSockets. This has not been observed in development and may potentially be because of the Wifi signal or the mobile devices themselves.

They gave a two and three out of five for the rating of the UI, with the person who had no issues with UI in previous question giving it a middling score of three. This would suggest some issues with the UI on mobile although with such a small sample this is hard to determine. Overall their final scores were three and one out of five. 

The user who had a bad experience gave information that they were using a Samsung phone with the default browser, which had not been tested against during development. Testing against all phone variations during development would be extremely hard, and for this more user testing would be recommended.
\section{Comments} 
A number of extra comments were left, of which all the desktop responses stated that some form of feedback once they hit submit would have been nice.

The mobile users also left comments stating the same, but also that the size of buttons could be scaled down.

\section{Lecturer results}
The lecturer questionnaire covered more functionality as it included both the running of tests in the lecture and the creation of quizzes. The responses about the backend concerning quiz creation, adding slides and the UI were all positive, either four or five out of five.

The responses concerning the running of quizzes were similarly positive with mostly four and five ratings for the UI, quiz controls, update times, and results. There was a rating of three given for the ease of running a quiz from the backend. Overall, they gave a four out of five for their experience.

There were a number of comments submitted in addition to the ratings, primarily about potential future work. This included adding authentication via university credentials, some feedback for when an answer is submitted as students requested and quiz cloning functionality. 

They also left a very positive comment which backs up their final rating of four out of five: "Overall, very nice system that I think I could use. It's good that PDF slides can be interleaved with the quiz questions."